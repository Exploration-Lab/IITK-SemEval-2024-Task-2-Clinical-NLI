{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmDjXiIdx1qc",
        "outputId": "0d165f53-4d48-485f-8a6a-edae77fb2aa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-7SSUi79kL0",
        "outputId": "98a04bb6-2969-4b90-c2a0-35d588811caa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIPFNnNhkuti",
        "outputId": "dea25ec9-407f-4832-a556-0cbebdaec8c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, TrainingArguments, Trainer\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "g3cpvIZ-ki9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BioLinkBERT tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained('michiyasunaga/BioLinkBERT-base')\n",
        "model = AutoModelForSequenceClassification.from_pretrained('michiyasunaga/BioLinkBERT-base', num_labels=2)\n",
        "\n",
        "# Other Models\n",
        "# allenai/scibert_scivocab_uncased\n",
        "# domenicrosati/ClinicalTrialBioBert-NLI4CT\n",
        "# medicalai/ClinicalBERT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66UHEtwxkp2S",
        "outputId": "2cad1836-10fc-43fd-c839-22009899cdea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at michiyasunaga/BioLinkBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDE36-UmzBaS",
        "outputId": "19be0c0a-4ac7-45b4-9fa7-cb23d402ce5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive/MyDrive/NLI4CT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXXo0jojzsZr",
        "outputId": "d164d92e-a900-440d-8dd8-e6b31ceb4432"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NLI4CT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "data = json.load(open('./train.json'))\n",
        "CT_files = os.listdir(\"./CT json\")\n",
        "# CT_files.remove(\".DS_Store\")\n",
        "\n",
        "CT_files_data = {file[:-5]:json.load(open(f\"./CT json/{file}\")) for file in CT_files}\n",
        "\n",
        "data_expanded = []\n",
        "for _id, value in data.items():\n",
        "    temp = {}\n",
        "    temp[\"id\"] = _id\n",
        "    p_nctid = value[\"Primary_id\"]\n",
        "    s_nctid = value.get(\"Secondary_id\")\n",
        "    section_id = value[\"Section_id\"]\n",
        "    statement = value[\"Statement\"]\n",
        "    primary_evidence = CT_files_data[p_nctid][section_id]\n",
        "    temp[\"statement\"] = statement\n",
        "    temp[\"primary_evidence\"] = primary_evidence\n",
        "    temp[\"label\"] = value[\"Label\"]\n",
        "\n",
        "    if s_nctid is not None:\n",
        "        secondary_evidence = CT_files_data[s_nctid][section_id]\n",
        "        temp[\"secondary_evidence\"] = secondary_evidence\n",
        "\n",
        "    data_expanded.append(temp)\n",
        "\n",
        "data_expanded[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwPdrVfszyX6",
        "outputId": "dad20100-de85-4c88-e182-13fb003bf953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '5bc844fc-e852-4270-bfaf-36ea9eface3d',\n",
              " 'statement': 'All the primary trial participants do not receive any oral capecitabine, oral lapatinib ditosylate or cixutumumab IV, in conrast all the secondary trial subjects receive these.',\n",
              " 'primary_evidence': ['INTERVENTION 1: ',\n",
              "  '  Diagnostic (FLT PET)',\n",
              "  '  Patients with early stage, ER positive primary breast cancer undergo FLT PET scan at baseline and 1-6 weeks after the start of standard endocrine treatment. The surgery follows 1-7 days after the second FLT PET scan.',\n",
              "  '  Tracer used in the FLT PET (positron emission tomography) scanning procedure: [F18] fluorothymidine.',\n",
              "  '  Positron Emission Tomography: Undergo FLT PET',\n",
              "  '  Laboratory Biomarker Analysis: Correlative studies - Ki67 staining of the tumor tissue in the biopsy and surgical specimen.'],\n",
              " 'label': 'Contradiction',\n",
              " 'secondary_evidence': ['INTERVENTION 1: ',\n",
              "  '  Arm A',\n",
              "  '  Patients receive oral capecitabine twice daily on days 1-14 and oral lapatinib ditosylate once daily on days 1-21. Courses repeat every 21 days in the absence of disease progression or unacceptable toxicity. lapatinib ditosylate: Given PO and capecitabine: Given PO',\n",
              "  'INTERVENTION 2: ',\n",
              "  '  Arm B',\n",
              "  '  Patients receive capecitabine and lapatinib ditosylate as in arm I. Patients also receive cixutumumab IV over 1-1½ hours on days 1, 8, and 15. Courses repeat every 21 days in the absence of disease progression or unacceptable toxicity. cixutumumab: Given IV, lapatinib ditosylate: Given PO and capecitabine: Given PO']}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samples = []\n",
        "for sample in data_expanded:\n",
        "    primary_evidence = \"\".join(sample['primary_evidence'])\n",
        "    sentence = f\"Primary trial evidence are \\n {primary_evidence}\"\n",
        "    secondary_evidence = sample.get(\"secondary_evidence\")\n",
        "    if secondary_evidence:\n",
        "        secondary_evidence = \"\".join(sample['secondary_evidence'])\n",
        "        sentence = f\"{sentence}\\n Secondary trial evidence are \\n{secondary_evidence}\"\n",
        "    temp = {\"id\": sample['id'], \"clinical_trial\":sentence, \"hypothesis\":sample['statement'], \"label\":sample['label']}\n",
        "    samples.append(temp)\n",
        "\n",
        "samples[80]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJ4lOiFR0KaJ",
        "outputId": "896362d3-2547-4b0e-873d-36a23ca7e99c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'b6ac985d-87ae-4e0f-83e1-38033c1db5cc',\n",
              " 'clinical_trial': 'Primary trial evidence are \\n Inclusion Criteria:  Pre-menopausal women aged 18 years or over with histologically/cytologically-confirmed oestrogen receptor positive (ER +ve) breast cancer  World Health Organization (WHO) performance status of 0, 1, or 2  Provided written informed consentExclusion Criteria:  Treatment with tamoxifen or other hormonal therapies as early breast cancer (EBC) adjuvant in the previous 24 weeks  Received radiotherapy within the past 4 weeks  History of systemic malignancy other than breast cancer within the previous 3 years  Estimated survival less than 24 weeks\\n Secondary trial evidence are \\nInclusion Criteria:  Recurrent or residual metastatic breast carcinoma  Zubrod performance status less than 2  18-60 years old  Related donor human leukocyte antigen (HLA)-compatible for allogeneic transplantation or unrelated HLA-compatible donor.  No major organ dysfunction or active infectionExclusion Criteria: None',\n",
              " 'hypothesis': \"the primary trial and the secondary trial use ECOG to evaluate potential candidates' performance status.\",\n",
              " 'label': 'Contradiction'}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "premises = []\n",
        "hypotheses = []\n",
        "label = []\n",
        "for sample in samples:\n",
        "  premises.append(sample['clinical_trial'])\n",
        "  hypotheses.append(sample['hypothesis'])\n",
        "  label.append(1 if sample['label'] == \"Entailment\" else 0)"
      ],
      "metadata": {
        "id": "D57V_tYKmacw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize input data and create input tensors\n",
        "input_ids_list = []\n",
        "attention_masks_list = []\n",
        "\n",
        "# Tokenize input data and pad to the same length\n",
        "max_length = 512\n",
        "\n",
        "for premise, hypothesis in zip(premises, hypotheses):\n",
        "    encoding = tokenizer(premise, hypothesis, padding='max_length', truncation=True, return_tensors=\"pt\", max_length=max_length)\n",
        "    input_ids_list.append(encoding[\"input_ids\"])\n",
        "    attention_masks_list.append(encoding[\"attention_mask\"])\n",
        "\n",
        "input_ids = torch.stack(input_ids_list)\n",
        "attention_masks = torch.stack(attention_masks_list)\n",
        "labels = torch.tensor(label)"
      ],
      "metadata": {
        "id": "Ty4yhgF9nYLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define custom model (modify this for your specific task)\n",
        "class BERTModel(torch.nn.Module):\n",
        "    def __init__(self, pretrained_model, num_labels):\n",
        "        super().__init__()\n",
        "        self.pretrained_model = pretrained_model\n",
        "        self.classifier = torch.nn.Linear(pretrained_model.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.pretrained_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # print(outputs)\n",
        "        # pooled_output = outputs.pooler_output  # Use the pooled_output as the final representation\n",
        "        logits = self.classifier(outputs.logits)\n",
        "        return logits\n",
        "\n",
        "bert_model = BERTModel(model, num_labels=2)"
      ],
      "metadata": {
        "id": "tJX4GgoPn6Yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into train and validation sets\n",
        "train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n",
        "    input_ids, attention_masks, labels, test_size=0.15, random_state=42\n",
        ")\n",
        "\n",
        "# Create DataLoader objects for training and validation\n",
        "train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "val_dataset = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8)"
      ],
      "metadata": {
        "id": "DYGgS9Pnp05G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtNEwdgbvyRO",
        "outputId": "9c9c7ecc-cc00-45ca-86cf-f9c7907f7ad0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(28895, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(data_loader):\n",
        "  model.eval()\n",
        "  total_val_loss = 0\n",
        "  true_labels = []\n",
        "  predicted_labels = []\n",
        "\n",
        "  from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for batch in data_loader:\n",
        "          input_ids_batch, attention_mask_batch, labels_batch = batch\n",
        "\n",
        "          input_ids_batch = input_ids_batch.squeeze(1)\n",
        "          attention_mask_batch = attention_mask_batch.squeeze(1)\n",
        "          input_ids_batch = input_ids_batch.to(device)\n",
        "          attention_mask_batch = attention_mask_batch.to(device)\n",
        "          labels_batch = labels_batch.to(device)\n",
        "\n",
        "          # Forward pass\n",
        "          outputs = model(input_ids_batch, attention_mask=attention_mask_batch)\n",
        "          logits = outputs.logits  # Logits for classification\n",
        "\n",
        "          # Pass logits to the loss function\n",
        "          loss = loss_fn(logits, labels_batch)\n",
        "          total_val_loss += loss.item()\n",
        "\n",
        "          # Calculate predicted labels\n",
        "          predicted_batch = torch.argmax(logits, dim=1)\n",
        "\n",
        "          true_labels.extend(labels_batch.cpu().numpy())  # Convert to NumPy array\n",
        "          predicted_labels.extend(predicted_batch.cpu().numpy())  # Convert to NumPy array\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "  average_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "  # Calculate precision, recall, and F1 score\n",
        "  precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
        "  recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
        "  f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "  print(f\"Loss: {average_val_loss}\", end=\" ... \")\n",
        "  print(f\"Accuracy: {accuracy}\", end=\" ... \")\n",
        "  print(f\"Precision: {precision}\", end=\" ... \")\n",
        "  print(f\"Recall: {recall}\", end=\" ... \")\n",
        "  print(f\"F1 Score: {f1}\")\n",
        "  return average_val_loss, accuracy, precision, recall, f1\n"
      ],
      "metadata": {
        "id": "-mwSbLqb75VZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define optimizer and loss function\n",
        "optimizer = AdamW(model.parameters(), lr=1e-7)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "train_acc = []\n",
        "val_acc = []\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "train_prec = []\n",
        "val_prec = []\n",
        "train_rec = []\n",
        "val_rec = []\n",
        "train_f1 = []\n",
        "val_f1 = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(50):\n",
        "    print(f\"\\n######### {epoch} ###########\")\n",
        "    model.train()\n",
        "    print(\"Training Metrics\")\n",
        "    l, a, p, r, f = evaluate(train_loader)\n",
        "    train_acc.append(a)\n",
        "    train_loss.append(l)\n",
        "    train_prec.append(p)\n",
        "    train_rec.append(r)\n",
        "    train_f1.append(f)\n",
        "    print(\"Validation Metrics\")\n",
        "    l, a, p, r, f = evaluate(val_loader)\n",
        "    val_acc.append(a)\n",
        "    val_loss.append(l)\n",
        "    val_prec.append(p)\n",
        "    val_rec.append(r)\n",
        "    val_f1.append(f)\n",
        "    for batch in train_loader:\n",
        "        input_ids_batch, attention_mask_batch, labels_batch = batch\n",
        "        input_ids_batch = input_ids_batch.squeeze(1)\n",
        "        attention_mask_batch = attention_mask_batch.squeeze(1)\n",
        "        input_ids_batch = input_ids_batch.to(device)\n",
        "        attention_mask_batch = attention_mask_batch.to(device)\n",
        "        labels_batch = labels_batch.to(device)\n",
        "        # print(input_ids_batch.shape)\n",
        "        # print(attention_mask_batch.shape)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_ids=input_ids_batch, attention_mask=attention_mask_batch).logits\n",
        "        loss = loss_fn(logits, labels_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    torch.save(model.state_dict(), f'bioLinkBert{epoch}.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLZG2uLOsI-g",
        "outputId": "d50e9a49-fb46-4817-af4e-7166f24a4824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "######### 0 ###########\n",
            "Training Metrics\n",
            "Loss: 3.9915558006614447 ... Accuracy: 0.4955017301038062 ... Precision: 0.492390821868768 ... Recall: 0.4955017301038062 ... F1 Score: 0.46598520205149063\n",
            "Validation Metrics\n",
            "Loss: 0.7051220573484898 ... Accuracy: 0.5098039215686274 ... Precision: 0.5262459150326798 ... Recall: 0.5098039215686274 ... F1 Score: 0.4767650650003592\n",
            "\n",
            "######### 1 ###########\n",
            "Training Metrics\n",
            "Loss: 3.9640215430408716 ... Accuracy: 0.4996539792387543 ... Precision: 0.4985619967156685 ... Recall: 0.4996539792387543 ... F1 Score: 0.4871571605002031\n",
            "Validation Metrics\n",
            "Loss: 0.6991189736872911 ... Accuracy: 0.5254901960784314 ... Precision: 0.5348951655929733 ... Recall: 0.5254901960784314 ... F1 Score: 0.5146283799652414\n",
            "\n",
            "######### 2 ###########\n",
            "Training Metrics\n",
            "Loss: 3.9513072166591883 ... Accuracy: 0.5031141868512111 ... Precision: 0.5025409807416729 ... Recall: 0.5031141868512111 ... F1 Score: 0.4962643390838292\n",
            "Validation Metrics\n",
            "Loss: 0.6969400644302368 ... Accuracy: 0.5098039215686274 ... Precision: 0.5144165739877965 ... Recall: 0.5098039215686274 ... F1 Score: 0.5054044828763226\n",
            "\n",
            "######### 3 ###########\n",
            "Training Metrics\n",
            "Loss: 3.9432539772242308 ... Accuracy: 0.5017301038062284 ... Precision: 0.5012327041814029 ... Recall: 0.5017301038062284 ... F1 Score: 0.49776095719040003\n",
            "Validation Metrics\n",
            "Loss: 0.6963593363761902 ... Accuracy: 0.49411764705882355 ... Precision: 0.49747930998618184 ... Recall: 0.49411764705882355 ... F1 Score: 0.49099342187121103\n",
            "\n",
            "######### 4 ###########\n",
            "Training Metrics\n",
            "Loss: 3.9369767997413874 ... Accuracy: 0.5031141868512111 ... Precision: 0.502666682969176 ... Recall: 0.5031141868512111 ... F1 Score: 0.4992794448953203\n",
            "Validation Metrics\n",
            "Loss: 0.6963015161454678 ... Accuracy: 0.49411764705882355 ... Precision: 0.49721222157330947 ... Recall: 0.49411764705882355 ... F1 Score: 0.4916049734459796\n",
            "\n",
            "######### 5 ###########\n",
            "Training Metrics\n",
            "Loss: 3.931490669026971 ... Accuracy: 0.5065743944636678 ... Precision: 0.5062310488209124 ... Recall: 0.5065743944636678 ... F1 Score: 0.502705344786068\n",
            "Validation Metrics\n",
            "Loss: 0.6965340357273817 ... Accuracy: 0.4980392156862745 ... Precision: 0.5016737554842121 ... Recall: 0.4980392156862745 ... F1 Score: 0.49461196243203165\n",
            "\n",
            "######### 6 ###########\n",
            "Training Metrics\n",
            "Loss: 3.926669327542186 ... Accuracy: 0.510726643598616 ... Precision: 0.5105093668087871 ... Recall: 0.510726643598616 ... F1 Score: 0.5071291999465999\n",
            "Validation Metrics\n",
            "Loss: 0.6969284359365702 ... Accuracy: 0.4980392156862745 ... Precision: 0.5016737554842121 ... Recall: 0.4980392156862745 ... F1 Score: 0.49461196243203165\n",
            "\n",
            "######### 7 ###########\n",
            "Training Metrics\n",
            "Loss: 3.922541232779622 ... Accuracy: 0.5162629757785467 ... Precision: 0.5161935366413677 ... Recall: 0.5162629757785467 ... F1 Score: 0.5129348242365356\n",
            "Validation Metrics\n",
            "Loss: 0.6974098272621632 ... Accuracy: 0.49019607843137253 ... Precision: 0.4935530246035352 ... Recall: 0.49019607843137253 ... F1 Score: 0.4867152743450321\n",
            "\n",
            "######### 8 ###########\n",
            "Training Metrics\n",
            "Loss: 3.918591871857643 ... Accuracy: 0.5204152249134948 ... Precision: 0.5204789291031828 ... Recall: 0.5204152249134948 ... F1 Score: 0.5168890177694396\n",
            "Validation Metrics\n",
            "Loss: 0.6979570873081684 ... Accuracy: 0.49411764705882355 ... Precision: 0.49801842556133336 ... Recall: 0.49411764705882355 ... F1 Score: 0.489577426328365\n",
            "\n",
            "######### 9 ###########\n",
            "Training Metrics\n",
            "Loss: 3.915114400908351 ... Accuracy: 0.5273356401384083 ... Precision: 0.5276360675758193 ... Recall: 0.5273356401384083 ... F1 Score: 0.523629383574873\n",
            "Validation Metrics\n",
            "Loss: 0.6985624115914106 ... Accuracy: 0.4980392156862745 ... Precision: 0.5019687752839291 ... Recall: 0.4980392156862745 ... F1 Score: 0.4939095123388958\n",
            "\n",
            "######### 10 ###########\n",
            "Training Metrics\n",
            "Loss: 3.911661423742771 ... Accuracy: 0.532871972318339 ... Precision: 0.533296937767406 ... Recall: 0.532871972318339 ... F1 Score: 0.5294373549702334\n",
            "Validation Metrics\n",
            "Loss: 0.6991851590573788 ... Accuracy: 0.49019607843137253 ... Precision: 0.4942846299810247 ... Recall: 0.49019607843137253 ... F1 Score: 0.4843784201847447\n",
            "\n",
            "######### 11 ###########\n",
            "Training Metrics\n",
            "Loss: 3.908434310927987 ... Accuracy: 0.5273356401384083 ... Precision: 0.5275495302642205 ... Recall: 0.5273356401384083 ... F1 Score: 0.5241925105273245\n",
            "Validation Metrics\n",
            "Loss: 0.6998248565942049 ... Accuracy: 0.47843137254901963 ... Precision: 0.48192559074912017 ... Recall: 0.47843137254901963 ... F1 Score: 0.4720219270503901\n",
            "\n",
            "######### 12 ###########\n",
            "Training Metrics\n",
            "Loss: 3.9053779542446136 ... Accuracy: 0.5307958477508651 ... Precision: 0.5311501948499552 ... Recall: 0.5307958477508651 ... F1 Score: 0.5274020995001779\n",
            "Validation Metrics\n",
            "Loss: 0.7004822604358196 ... Accuracy: 0.4745098039215686 ... Precision: 0.477707779886148 ... Recall: 0.4745098039215686 ... F1 Score: 0.46851314080581374\n",
            "\n",
            "######### 13 ###########\n",
            "Training Metrics\n",
            "Loss: 3.9024873431771994 ... Accuracy: 0.5273356401384083 ... Precision: 0.5275827397440962 ... Recall: 0.5273356401384083 ... F1 Score: 0.5239729389074256\n",
            "Validation Metrics\n",
            "Loss: 0.7010969072580338 ... Accuracy: 0.4745098039215686 ... Precision: 0.477707779886148 ... Recall: 0.4745098039215686 ... F1 Score: 0.46851314080581374\n",
            "\n",
            "######### 14 ###########\n",
            "Training Metrics\n",
            "Loss: 3.899629570543766 ... Accuracy: 0.5294117647058824 ... Precision: 0.5296196910136604 ... Recall: 0.5294117647058824 ... F1 Score: 0.5266469340772746\n",
            "Validation Metrics\n",
            "Loss: 0.7017918657511473 ... Accuracy: 0.47058823529411764 ... Precision: 0.47351920065956177 ... Recall: 0.47058823529411764 ... F1 Score: 0.46499398011478343\n",
            "\n",
            "######### 15 ###########\n",
            "Training Metrics\n",
            "Loss: 3.8971226792782545 ... Accuracy: 0.5266435986159169 ... Precision: 0.5268166715513841 ... Recall: 0.5266435986159169 ... F1 Score: 0.5236558254089383\n",
            "Validation Metrics\n",
            "Loss: 0.7025875113904476 ... Accuracy: 0.47058823529411764 ... Precision: 0.47351920065956177 ... Recall: 0.47058823529411764 ... F1 Score: 0.46499398011478343\n",
            "\n",
            "######### 16 ###########\n",
            "Training Metrics\n",
            "Loss: 3.895530052483082 ... Accuracy: 0.5314878892733564 ... Precision: 0.5317905249604852 ... Recall: 0.5314878892733564 ... F1 Score: 0.5284783859567592\n",
            "Validation Metrics\n",
            "Loss: 0.7033417262136936 ... Accuracy: 0.4627450980392157 ... Precision: 0.465261820299677 ... Recall: 0.4627450980392157 ... F1 Score: 0.4570679650053728\n",
            "\n",
            "######### 17 ###########\n",
            "Training Metrics\n",
            "Loss: 3.8922847751528025 ... Accuracy: 0.5356401384083045 ... Precision: 0.5358993605029119 ... Recall: 0.5356401384083045 ... F1 Score: 0.5333411335036725\n",
            "Validation Metrics\n",
            "Loss: 0.7038062978535891 ... Accuracy: 0.47058823529411764 ... Precision: 0.47342120292163875 ... Recall: 0.47058823529411764 ... F1 Score: 0.46583684150642846\n",
            "\n",
            "######### 18 ###########\n",
            "Training Metrics\n",
            "Loss: 3.8898934181779623 ... Accuracy: 0.5411764705882353 ... Precision: 0.5415422151225294 ... Recall: 0.5411764705882353 ... F1 Score: 0.5389048755781444\n",
            "Validation Metrics\n",
            "Loss: 0.704442173242569 ... Accuracy: 0.47058823529411764 ... Precision: 0.47342120292163875 ... Recall: 0.47058823529411764 ... F1 Score: 0.46583684150642846\n",
            "\n",
            "######### 19 ###########\n",
            "Training Metrics\n",
            "Loss: 3.8875097930431366 ... Accuracy: 0.5446366782006921 ... Precision: 0.5451091100168585 ... Recall: 0.5446366782006921 ... F1 Score: 0.5422459672444132\n",
            "Validation Metrics\n",
            "Loss: 0.7049838472157717 ... Accuracy: 0.4627450980392157 ... Precision: 0.46516526610644254 ... Recall: 0.4627450980392157 ... F1 Score: 0.4587095208092974\n",
            "\n",
            "######### 20 ###########\n",
            "Training Metrics\n",
            "Loss: 3.885054936632514 ... Accuracy: 0.5453287197231834 ... Precision: 0.5458873628932356 ... Recall: 0.5453287197231834 ... F1 Score: 0.5427059056685781\n",
            "Validation Metrics\n",
            "Loss: 0.7057349570095539 ... Accuracy: 0.4588235294117647 ... Precision: 0.4611303344867359 ... Recall: 0.4588235294117647 ... F1 Score: 0.4535444947209653\n",
            "\n",
            "######### 21 ###########\n",
            "Training Metrics\n",
            "Loss: 3.8829616382718086 ... Accuracy: 0.5487889273356401 ... Precision: 0.5494756564357246 ... Recall: 0.5487889273356401 ... F1 Score: 0.5460403354420478\n",
            "Validation Metrics\n",
            "Loss: 0.7063441202044487 ... Accuracy: 0.4392156862745098 ... Precision: 0.44062490606871263 ... Recall: 0.4392156862745098 ... F1 Score: 0.4341827284105132\n",
            "\n",
            "######### 22 ###########\n",
            "Training Metrics\n",
            "Loss: 3.880424316972494 ... Accuracy: 0.5480968858131487 ... Precision: 0.5487179328874301 ... Recall: 0.5480968858131487 ... F1 Score: 0.5454900401850555\n",
            "Validation Metrics\n",
            "Loss: 0.7070795353502035 ... Accuracy: 0.43137254901960786 ... Precision: 0.43242583185548106 ... Recall: 0.43137254901960786 ... F1 Score: 0.4262692001365343\n",
            "\n",
            "######### 23 ###########\n",
            "Training Metrics\n",
            "Loss: 3.8781555369496346 ... Accuracy: 0.5508650519031142 ... Precision: 0.5515143873663275 ... Recall: 0.5508650519031142 ... F1 Score: 0.5483686567247215\n",
            "Validation Metrics\n",
            "Loss: 0.7077926807105541 ... Accuracy: 0.4392156862745098 ... Precision: 0.4407282913165267 ... Recall: 0.4392156862745098 ... F1 Score: 0.435003368435982\n",
            "\n",
            "######### 24 ###########\n",
            "Training Metrics\n",
            "Loss: 3.8764270208775997 ... Accuracy: 0.5494809688581315 ... Precision: 0.5501332178845274 ... Recall: 0.5494809688581315 ... F1 Score: 0.5468821074432944\n",
            "Validation Metrics\n",
            "Loss: 0.7086353190243244 ... Accuracy: 0.43137254901960786 ... Precision: 0.4327035941934238 ... Recall: 0.43137254901960786 ... F1 Score: 0.42786082303353185\n",
            "\n",
            "######### 25 ###########\n",
            "Training Metrics\n",
            "Loss: 3.8738117311149836 ... Accuracy: 0.5522491349480969 ... Precision: 0.5529637878787221 ... Recall: 0.5522491349480969 ... F1 Score: 0.5496662419597718\n",
            "Validation Metrics\n",
            "Loss: 0.7093291375786066 ... Accuracy: 0.4196078431372549 ... Precision: 0.4207707910750507 ... Recall: 0.4196078431372549 ... F1 Score: 0.41704844760156434\n",
            "\n",
            "######### 26 ###########\n",
            "Training Metrics\n",
            "Loss: 3.8711983244866133 ... Accuracy: 0.558477508650519 ... Precision: 0.5591576940819463 ... Recall: 0.558477508650519 ... F1 Score: 0.5563347321879274\n",
            "Validation Metrics\n",
            "Loss: 0.709866140037775 ... Accuracy: 0.4196078431372549 ... Precision: 0.4207707910750507 ... Recall: 0.4196078431372549 ... F1 Score: 0.41704844760156434\n",
            "\n",
            "######### 27 ###########\n",
            "Training Metrics\n",
            "Loss: 3.8689121454954147 ... Accuracy: 0.5605536332179931 ... Precision: 0.5612922062911911 ... Recall: 0.5605536332179931 ... F1 Score: 0.5583779728387958\n",
            "Validation Metrics\n",
            "Loss: 0.710622027516365 ... Accuracy: 0.4196078431372549 ... Precision: 0.4207707910750507 ... Recall: 0.4196078431372549 ... F1 Score: 0.41704844760156434\n",
            "\n",
            "######### 28 ###########\n",
            "Training Metrics\n",
            "Loss: 3.866250954568386 ... Accuracy: 0.5626297577854671 ... Precision: 0.563428615883131 ... Recall: 0.5626297577854671 ... F1 Score: 0.5604211873503557\n",
            "Validation Metrics\n",
            "Loss: 0.7114127893000841 ... Accuracy: 0.4196078431372549 ... Precision: 0.4207707910750507 ... Recall: 0.4196078431372549 ... F1 Score: 0.41704844760156434\n",
            "\n",
            "######### 29 ###########\n",
            "Training Metrics\n",
            "Loss: 3.864165835082531 ... Accuracy: 0.5626297577854671 ... Precision: 0.5633488404538353 ... Recall: 0.5626297577854671 ... F1 Score: 0.5605913510893017\n",
            "Validation Metrics\n",
            "Loss: 0.7120753899216652 ... Accuracy: 0.4117647058823529 ... Precision: 0.41273833671399596 ... Recall: 0.4117647058823529 ... F1 Score: 0.40917072392050446\n",
            "\n",
            "######### 30 ###########\n",
            "Training Metrics\n",
            "Loss: 3.8625926412642 ... Accuracy: 0.5674740484429066 ... Precision: 0.5683457745657131 ... Recall: 0.5674740484429066 ... F1 Score: 0.5653326504318857\n",
            "Validation Metrics\n",
            "Loss: 0.7130645923316479 ... Accuracy: 0.4 ... Precision: 0.4007882882882883 ... Recall: 0.4 ... F1 Score: 0.3976695796920516\n",
            "\n",
            "######### 31 ###########\n",
            "Training Metrics\n",
            "Loss: 3.8600936848670244 ... Accuracy: 0.5660899653979239 ... Precision: 0.5670695542741478 ... Recall: 0.5660899653979239 ... F1 Score: 0.5636781937849005\n",
            "Validation Metrics\n",
            "Loss: 0.7136650364845991 ... Accuracy: 0.403921568627451 ... Precision: 0.4047058823529412 ... Recall: 0.403921568627451 ... F1 Score: 0.40129300023944453\n",
            "\n",
            "######### 32 ###########\n",
            "Training Metrics\n",
            "Loss: 3.85759212449193 ... Accuracy: 0.568166089965398 ... Precision: 0.5692149933296771 ... Recall: 0.568166089965398 ... F1 Score: 0.5657206508915069\n",
            "Validation Metrics\n",
            "Loss: 0.7143431827425957 ... Accuracy: 0.396078431372549 ... Precision: 0.39667342799188643 ... Recall: 0.396078431372549 ... F1 Score: 0.39341527655838454\n",
            "\n",
            "######### 33 ###########\n",
            "Training Metrics\n",
            "Loss: 3.8552370592951775 ... Accuracy: 0.5709342560553633 ... Precision: 0.5720961971870641 ... Recall: 0.5709342560553633 ... F1 Score: 0.5684133810704564\n",
            "Validation Metrics\n",
            "Loss: 0.7151876091957092 ... Accuracy: 0.396078431372549 ... Precision: 0.39667342799188643 ... Recall: 0.396078431372549 ... F1 Score: 0.39341527655838454\n",
            "\n",
            "######### 34 ###########\n",
            "Training Metrics\n",
            "Loss: 3.852789891883731 ... Accuracy: 0.572318339100346 ... Precision: 0.5735120807261775 ... Recall: 0.572318339100346 ... F1 Score: 0.5698055959702291\n",
            "Validation Metrics\n",
            "Loss: 0.7159764263778925 ... Accuracy: 0.39215686274509803 ... Precision: 0.392525264473013 ... Recall: 0.39215686274509803 ... F1 Score: 0.38913775879168094\n",
            "\n",
            "######### 35 ###########\n",
            "Training Metrics\n",
            "Loss: 3.8507473170757294 ... Accuracy: 0.5730103806228374 ... Precision: 0.5741400846867935 ... Recall: 0.5730103806228374 ... F1 Score: 0.5706370742668001\n",
            "Validation Metrics\n",
            "Loss: 0.716532101854682 ... Accuracy: 0.403921568627451 ... Precision: 0.40449068516295406 ... Recall: 0.403921568627451 ... F1 Score: 0.4006100217864924\n",
            "\n",
            "######### 36 ###########\n",
            "Training Metrics\n",
            "Loss: 3.8480606749653816 ... Accuracy: 0.5730103806228374 ... Precision: 0.5740883567084758 ... Recall: 0.5730103806228374 ... F1 Score: 0.5707252042935961\n",
            "Validation Metrics\n",
            "Loss: 0.7174616772681475 ... Accuracy: 0.4 ... Precision: 0.4003157362970447 ... Recall: 0.4 ... F1 Score: 0.39629452361469214\n",
            "\n",
            "######### 37 ###########\n",
            "Training Metrics\n",
            "Loss: 3.845087232068181 ... Accuracy: 0.5743944636678201 ... Precision: 0.5755541907693226 ... Recall: 0.5743944636678201 ... F1 Score: 0.5720288503631799\n",
            "Validation Metrics\n",
            "Loss: 0.7182529512792826 ... Accuracy: 0.4 ... Precision: 0.4003157362970447 ... Recall: 0.4 ... F1 Score: 0.39629452361469214\n",
            "\n",
            "######### 38 ###########\n",
            "Training Metrics\n",
            "Loss: 3.842489628121257 ... Accuracy: 0.5757785467128028 ... Precision: 0.5769142593015685 ... Recall: 0.5757785467128028 ... F1 Score: 0.5735081851409634\n",
            "Validation Metrics\n",
            "Loss: 0.7190297227352858 ... Accuracy: 0.4 ... Precision: 0.4003157362970447 ... Recall: 0.4 ... F1 Score: 0.39629452361469214\n",
            "\n",
            "######### 39 ###########\n",
            "Training Metrics\n",
            "Loss: 3.83987426944077 ... Accuracy: 0.5799307958477509 ... Precision: 0.5813297376302232 ... Recall: 0.5799307958477509 ... F1 Score: 0.5774175514283376\n",
            "Validation Metrics\n",
            "Loss: 0.7199980728328228 ... Accuracy: 0.396078431372549 ... Precision: 0.396416344315504 ... Recall: 0.396078431372549 ... F1 Score: 0.392723311546841\n",
            "\n",
            "######### 40 ###########\n",
            "Training Metrics\n",
            "Loss: 3.837186899036169 ... Accuracy: 0.5847750865051903 ... Precision: 0.5862550325781986 ... Recall: 0.5847750865051903 ... F1 Score: 0.5823355300681834\n",
            "Validation Metrics\n",
            "Loss: 0.720985995605588 ... Accuracy: 0.396078431372549 ... Precision: 0.396416344315504 ... Recall: 0.396078431372549 ... F1 Score: 0.392723311546841\n",
            "\n",
            "######### 41 ###########\n",
            "Training Metrics\n",
            "Loss: 3.833891937509179 ... Accuracy: 0.5826989619377163 ... Precision: 0.5842923297695007 ... Recall: 0.5826989619377163 ... F1 Score: 0.5800184146704961\n",
            "Validation Metrics\n",
            "Loss: 0.7218812238425016 ... Accuracy: 0.396078431372549 ... Precision: 0.396416344315504 ... Recall: 0.396078431372549 ... F1 Score: 0.392723311546841\n",
            "\n",
            "######### 42 ###########\n",
            "Training Metrics\n",
            "Loss: 3.8305023573338985 ... Accuracy: 0.5882352941176471 ... Precision: 0.5898286665747791 ... Recall: 0.5882352941176471 ... F1 Score: 0.5857717349256357\n",
            "Validation Metrics\n",
            "Loss: 0.7228731364011765 ... Accuracy: 0.396078431372549 ... Precision: 0.396416344315504 ... Recall: 0.396078431372549 ... F1 Score: 0.392723311546841\n",
            "\n",
            "######### 43 ###########\n",
            "Training Metrics\n",
            "Loss: 3.827193832024932 ... Accuracy: 0.5896193771626298 ... Precision: 0.5912451547322051 ... Recall: 0.5896193771626298 ... F1 Score: 0.5871640988418521\n",
            "Validation Metrics\n",
            "Loss: 0.7238060310482979 ... Accuracy: 0.396078431372549 ... Precision: 0.396416344315504 ... Recall: 0.396078431372549 ... F1 Score: 0.392723311546841\n",
            "\n",
            "######### 44 ###########\n",
            "Training Metrics\n",
            "Loss: 3.8248099982738495 ... Accuracy: 0.5910034602076124 ... Precision: 0.5924567235635779 ... Recall: 0.5910034602076124 ... F1 Score: 0.5888145798014836\n",
            "Validation Metrics\n",
            "Loss: 0.724723182618618 ... Accuracy: 0.4 ... Precision: 0.4003157362970447 ... Recall: 0.4 ... F1 Score: 0.39629452361469214\n",
            "\n",
            "######### 45 ###########\n",
            "Training Metrics\n",
            "Loss: 3.8214989453554153 ... Accuracy: 0.5930795847750865 ... Precision: 0.59454251207284 ... Recall: 0.5930795847750865 ... F1 Score: 0.5909432047716032\n",
            "Validation Metrics\n",
            "Loss: 0.7257527466863394 ... Accuracy: 0.396078431372549 ... Precision: 0.396416344315504 ... Recall: 0.396078431372549 ... F1 Score: 0.392723311546841\n",
            "\n",
            "######### 46 ###########\n",
            "Training Metrics\n",
            "Loss: 3.8183836564421654 ... Accuracy: 0.5986159169550173 ... Precision: 0.6000525994060572 ... Recall: 0.5986159169550173 ... F1 Score: 0.5966679383526613\n",
            "Validation Metrics\n",
            "Loss: 0.7267466988414526 ... Accuracy: 0.396078431372549 ... Precision: 0.396416344315504 ... Recall: 0.396078431372549 ... F1 Score: 0.392723311546841\n",
            "\n",
            "######### 47 ###########\n",
            "Training Metrics\n",
            "Loss: 3.81518348492682 ... Accuracy: 0.5979238754325259 ... Precision: 0.5994506427906788 ... Recall: 0.5979238754325259 ... F1 Score: 0.5958534272186192\n",
            "Validation Metrics\n",
            "Loss: 0.7278238106518984 ... Accuracy: 0.3843137254901961 ... Precision: 0.38447242161914413 ... Recall: 0.3843137254901961 ... F1 Score: 0.38125566535673483\n",
            "\n",
            "######### 48 ###########\n",
            "Training Metrics\n",
            "Loss: 3.811977654695511 ... Accuracy: 0.5979238754325259 ... Precision: 0.59938147497361 ... Recall: 0.5979238754325259 ... F1 Score: 0.5959332318414811\n",
            "Validation Metrics\n",
            "Loss: 0.7289120275527239 ... Accuracy: 0.3843137254901961 ... Precision: 0.3841218073488552 ... Recall: 0.3843137254901961 ... F1 Score: 0.3805113739052724\n",
            "\n",
            "######### 49 ###########\n",
            "Training Metrics\n",
            "Loss: 3.808345887809992 ... Accuracy: 0.6 ... Precision: 0.6015327468230695 ... Recall: 0.6 ... F1 Score: 0.5979801365324459\n",
            "Validation Metrics\n",
            "Loss: 0.7299628499895334 ... Accuracy: 0.396078431372549 ... Precision: 0.39610425403541183 ... Recall: 0.396078431372549 ... F1 Score: 0.39195501730103804\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u3zI5V_NALUd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}